import os
import sys
sys.path.append(os.getcwd())
import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.env_util import make_vec_env

# å‡è®¾ CelesteGymEnv åœ¨å½“å‰è·¯å¾„ä¸‹å¯å¯¼å…¥
from RL.celeste_env import CelesteGymEnv  # ğŸ‘ˆ æ›¿æ¢ä¸ºå®é™…æ¨¡å—åï¼Œå¦‚ 'celeste_env'

# è®¾ç½®æ—¥å¿—å’Œæ¨¡å‹ä¿å­˜è·¯å¾„
log_dir = "./logs/celeste_ppo/"
model_dir = "./models/celeste_ppo/"
os.makedirs(log_dir, exist_ok=True)
os.makedirs(model_dir, exist_ok=True)

def make_env(goal=(64.0, 32.0), custom_room=None, rank=0,max_step=2000,level=0,randomize_start_position=False):
    """
    è¿”å›ä¸€ä¸ªç¯å¢ƒæ„é€ å‡½æ•°ï¼ˆç”¨äº VecEnvï¼‰
    """
    def _init():
        env = CelesteGymEnv(
            goal=goal,
            custom_room=custom_room,
            render_mode=None,
            max_step=max_step,
            level=level,
            randomize_start_position=randomize_start_position
        )
        env = Monitor(env, filename=os.path.join(log_dir, f"monitor_{rank}"))
        return env
    return _init

if __name__ == "__main__":
    # é…ç½®ç›®æ ‡ç‚¹ï¼ˆå¯æ ¹æ® curriculum æ”¹å˜ï¼‰
    RAND_POS=False
    TARGET_GOAL = (108, -1.0) # ç¤ºä¾‹ç›®æ ‡ï¼šæˆ¿é—´ä¸­æŸä¸ªä½ç½®
    LEVEL=0
    EVAL_GOAL=(108, -1.0)
    EVAL_LEVEL=0
    BEST_MODEL_PATH=None#'models/celeste_ppo/best/best_model.zip' #RL/finished_models/ppo/best_model.zip'
    total_timesteps = 1000_000# æ ¹æ®éš¾åº¦è°ƒæ•´
    CUSTOM_ROOM=None
    # åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒï¼ˆå³ä½¿1ä¸ªä¹Ÿæ¨'èç”¨ DummyVecEnvï¼‰
    num_envs = 8  # å¯å¢åŠ åˆ° 4/8 æå‡æ ·æœ¬æ•ˆç‡ï¼ˆéœ€ç¡®ä¿ PICO8 æ”¯æŒå¤šå®ä¾‹ï¼‰
    env = DummyVecEnv([make_env(goal=TARGET_GOAL, rank=i,max_step=1200,level=LEVEL,randomize_start_position=RAND_POS,custom_room=CUSTOM_ROOM) for i in range(num_envs)])

    # å¯é€‰ï¼šå¦‚æœ observation æ˜¯ Dictï¼ŒSB3 é»˜è®¤æ”¯æŒï¼Œä½†éœ€ç¡®è®¤ç½‘ç»œç»“æ„
    # PPO é»˜è®¤ä¼šè‡ªåŠ¨å¤„ç† spaces.Dictï¼ˆä½¿ç”¨ CombinedExtractorï¼‰

    # åˆ›å»º PPO æ¨¡å‹
    model = PPO(
        "MultiInputPolicy",  # è‡ªåŠ¨å¤„ç† Dict observation
        env,
        verbose=1,
        tensorboard_log=log_dir,
        learning_rate=1e-3,
        n_steps=2048,
        batch_size=256,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.01,
        vf_coef=0.5,
        max_grad_norm=0.5,
        seed=42,
    )
    if BEST_MODEL_PATH:
        print(F"load model from {BEST_MODEL_PATH}")
        model.load(BEST_MODEL_PATH)
    # å›è°ƒï¼šå®šæœŸä¿å­˜æ¨¡å‹ & è¯„ä¼°
    checkpoint_callback = CheckpointCallback(
        save_freq=10000,  # æ¯ 10k steps ä¿å­˜ä¸€æ¬¡
        save_path=model_dir,
        name_prefix="celeste_ppo"
    )

    # å¯é€‰ï¼šåˆ›å»ºç‹¬ç«‹è¯„ä¼°ç¯å¢ƒ
    eval_env = DummyVecEnv([make_env(goal=EVAL_GOAL, rank=999,max_step=1200,level=EVAL_LEVEL,randomize_start_position=RAND_POS,custom_room=CUSTOM_ROOM)])
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=os.path.join(model_dir, "best"),
        log_path=log_dir,
        eval_freq=5000,  # æ¯ 5k training steps è¯„ä¼°ä¸€æ¬¡
        deterministic=True,
        render=False,
        n_eval_episodes=10,
        verbose=1,
    )

    # å¼€å§‹è®­ç»ƒ
    
    print(f"Starting PPO training for {total_timesteps} timesteps... level: {LEVEL}")
    model.learn(
        total_timesteps=total_timesteps,
        callback=[checkpoint_callback, eval_callback],
        tb_log_name="ppo_celeste_run",
        progress_bar=True,
    )

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    model.save(os.path.join(model_dir, "celeste_ppo_final"))
    print("Training finished and model saved.")