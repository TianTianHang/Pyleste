import os
import sys
sys.path.append(os.getcwd())

import gymnasium as gym
from stable_baselines3 import DQN
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import SubprocVecEnv
# å‡è®¾ CelesteGymEnv åœ¨å½“å‰è·¯å¾„ä¸‹å¯å¯¼å…¥
from RL.celeste_env import CelesteGymEnv  # ğŸ‘ˆ æ›¿æ¢ä¸ºå®é™…æ¨¡å—å

# è®¾ç½®æ—¥å¿—å’Œæ¨¡å‹ä¿å­˜è·¯å¾„
log_dir = "./logs/celeste_dqn/"
model_dir = "./models/celeste_dqn/"
os.makedirs(log_dir, exist_ok=True)
os.makedirs(model_dir, exist_ok=True)

def make_env(goal=(64.0, 32.0), custom_room=None, rank=0,max_step=2000,level=0):
    """
    è¿”å›ä¸€ä¸ªç¯å¢ƒæ„é€ å‡½æ•°ï¼ˆç”¨äº VecEnvï¼‰
    """
    def _init():
        env = CelesteGymEnv(
            goal=goal,
            custom_room=custom_room,
            render_mode=None,
            max_step=max_step,
            level=level
        )
        env = Monitor(env, filename=os.path.join(log_dir, f"monitor_{rank}"))
        return env
    return _init

if __name__ == "__main__":
    # é…ç½®ç›®æ ‡ç‚¹
    TARGET_GOAL = (108, 0.0)
    LEVEL=0
    # æ³¨æ„ï¼šDQN ä¸æ”¯æŒ VecEnv çš„å¤šä¸ªå¹¶è¡Œç¯å¢ƒè¿›è¡Œç»éªŒå›æ”¾é‡‡æ ·ï¼ˆSB3 çš„ DQN åªèƒ½å¤„ç†å•ç¯å¢ƒæˆ– DummyVecEnv(n=1)ï¼‰
    # å› æ­¤æˆ‘ä»¬åªç”¨ 1 ä¸ªç¯å¢ƒï¼ˆDummyVecEnv ä»å¯ç”¨ï¼Œä½† n_envs=1ï¼‰
    num_envs = 1  # DQN åœ¨ SB3 ä¸­ä¸æ”¯æŒå¤šè¿›ç¨‹é‡‡æ ·åˆ° replay buffer
    env = DummyVecEnv([make_env(goal=TARGET_GOAL, rank=i,level=LEVEL) for i in range(num_envs)])

    # æ£€æŸ¥ action space æ˜¯å¦ä¸º Discrete
    assert isinstance(env.action_space, gym.spaces.Discrete), \
        "DQN only supports discrete action spaces!"

    # åˆ›å»º DQN æ¨¡å‹
    model = DQN(
        "MultiInputPolicy",  # è‡ªåŠ¨å¤„ç† Dict observationï¼ˆå¦‚ {"image": ..., "vector": ...}ï¼‰
        env,
        verbose=1,
        tensorboard_log=log_dir,
        learning_rate=1e-4,
        buffer_size=50000,          # ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°
        learning_starts=10000,      # å¼€å§‹å­¦ä¹ å‰çš„ warmup steps
        batch_size=32,
        tau=1.0,                    # ç¡¬æ›´æ–° target networkï¼ˆä¹Ÿå¯è®¾ä¸º 0.005 è½¯æ›´æ–°ï¼‰
        gamma=0.99,
        train_freq=4,               # æ¯ 4 æ­¥è®­ç»ƒä¸€æ¬¡
        gradient_steps=1,
        target_update_interval=1000,  # æ¯ 1000 æ­¥æ›´æ–° target net
        exploration_fraction=0.2,   # å‰ 20% timesteps è¿›è¡Œ epsilon è¡°å‡
        exploration_initial_eps=1.0,
        exploration_final_eps=0.05,
        max_grad_norm=10,
        seed=42,
    )

    # å›è°ƒï¼šå®šæœŸä¿å­˜æ¨¡å‹
    checkpoint_callback = CheckpointCallback(
        save_freq=10000,
        save_path=model_dir,
        name_prefix="celeste_dqn"
    )

    # è¯„ä¼°å›è°ƒï¼ˆä½¿ç”¨ç‹¬ç«‹ç¯å¢ƒï¼‰
    eval_env = DummyVecEnv([make_env(goal=TARGET_GOAL, rank=999,level=LEVEL)])
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=os.path.join(model_dir, "best"),
        log_path=log_dir,
        eval_freq=5000,
        deterministic=True,
        render=False,
        n_eval_episodes=5,
        verbose=1,
    )

    # å¼€å§‹è®­ç»ƒ
    total_timesteps = 500_000
    print(f"Starting DQN training for {total_timesteps} timesteps...")
    model.learn(
        total_timesteps=total_timesteps,
        callback=[checkpoint_callback, eval_callback],
        tb_log_name="dqn_celeste_run",
        progress_bar=True,
    )

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    model.save(os.path.join(model_dir, "celeste_dqn_final"))
    print("DQN training finished and model saved.")